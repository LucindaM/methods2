%% beamer/knitr slides 
%% for Statistical Modeling and Data Visualization course @ UMass
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../preambles/standard_knitr_beamer_preamble}

%        The following variables are assumed by the standard preamble:
%        Global variable containing module name:

\title{Introduction to Multiple Linear Regression}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{multRegression}
%	Global variable containing author name:
\author{Nicholas G Reich, Jeff Goldsmith}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\textunderscore US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}


\input{../preambles/shortcuts}

\hypersetup{colorlinks,linkcolor=,urlcolor=MainColor}


%	******	Document body begins here	**********************

\begin{document}

%	Title page
\begin{frame}[plain]
	\titlepage
\end{frame}

%	******	Everything through the above line must be placed at
%		the top of any TeX file using the statsTeachR standard
%		beamer preamble. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Today's lecture}

\bi
        \myitem Multiple Linear Regression 
	\bi
		\myitem Interpretation
		\myitem Notation
		%\myitem Estimation
	\ei
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Multiple linear regression model}

\bi
	\myitem Observe data $(y_i, x_{i1}, \ldots, x_{ip})$ for subjects $1, \ldots, n$. Want to estimate $\beta_0, \beta_1, \ldots, \beta_p$ in the model
	$$ y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_1x_{ip} + \epsilon_i; \mbox{ } \epsilon_i \stackrel{iid}{\sim} (0,\sigma^2)$$
	\myitem Assumptions (residuals have mean zero, constant variance, are independent) are as in SLR
	\myitem Impose linearity which (as in the SLR) is a big assumption
	\myitem Our primary interest will be $E(y | \bx)$
	\myitem Eventually estimate model parameters using least squares
\ei

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Interpretation of $\beta_1$}


\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Omitted variable bias}

What happens if we ignore $x_2$ and fit the simple linear regression:
$$ y_i = \beta^{*}_0 + \beta^{*}_1 x_{i,1} + \epsilon^{*}_i$$

Does $\beta^{*}_1 = \beta_{1}$? 

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Omitted variable bias}

\begin{block}{When should you be concerned?}
If both of the following conditions are met, then  $\beta^{*}_1 = \beta_1$:
\bi
        \myitem The omitted variable is unrelated to the outcome
	\myitem The omitted variable is uncorrelated with the retained variable
\ei
\end{block}

{\bf Extra credit for problem set 1}: create a simulation where you show an example of omitted variable bias. 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Matrix notation}

\bi
        \myitem Observe data $(y_i, x_{i1}, \ldots, x_{ip})$ for subjects $1, \ldots, n$. Want to estimate $\beta_0, \beta_1, \ldots, \beta_p$ in the model
	$$ y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_1x_{ip} + \epsilon_i; \mbox{ } \epsilon_i \stackrel{iid}{\sim} (0,\sigma^2)$$
	\myitem Notation is cumbersome. To fix this, let
	\bi
		\myitem $\bx_i = [1, x_{i1}, \ldots, x_{ip}]$
		\myitem $\bbeta^{T} = [\beta_0, \beta_1, \ldots, \bbeta_{p}]$
		\myitem Then $y_i = \bx_i \bbeta + \epsilon_i$
	\ei
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Matrix notation}

\bi
	\myitem Let
	\begin{displaymath}\tiny
	\by = \left[
		\begin{array}{c}
		y_1 \\
		\vdots \\
		y_n \\
		\end{array}
    		\right],\quad
	\bX = \left[
		\begin{array}{cccc}
		1 & x_{11} & \hdots & x_{1p} \\
		\vdots & \vdots & x_{ij} & \vdots \\
		1 & x_{n1} & \hdots & x_{np} \\
		\end{array}
		\right], \quad
	\bbeta = \left[
		\begin{array}{c}
		\beta_0 \\
		\vdots \\
		\beta_p \\
		\end{array}
		\right], \quad
         \bepsilon = \left[
		\begin{array}{c}
		\epsilon_1 \\
		\vdots \\
		\epsilon_n \\
		\end{array}
		\right]
	\end{displaymath}
	\bigskip
	\myitem Then we can write the model in a more compact form:
	$${\by}_{n \times 1} = {\bX}_{n \times (p+1)}{\bbeta}_{(p+1) \times 1} + {\bepsilon}_{n \times 1}$$
	\myitem $\bX$ is called the \emph{design matrix}
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Matrix notation}

$$\by = \bX\bbeta + \bepsilon$$

\bi
	\myitem $\epsilon$ is a random vector rather than a random variable
	\bigskip
	\myitem $E(\bepsilon) = 0$ and $Var(\bepsilon) = \sigma^2I$
	\bigskip
	\myitem Note that $Var$ is an abuse of notation; in the present context it really means the ``variance-covariance matrix''
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's big ideas}

\bi
        \myitem Multiple linear regression models, interpretation, notation, biases
\ei

\end{frame}




\end{document}