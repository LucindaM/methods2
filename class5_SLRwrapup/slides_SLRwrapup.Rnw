%% beamer/knitr slides 
%% for Statistical Modeling and Data Visualization course @ UMass
%% Nicholas Reich: nick [at] schoolph.umass.edu


\documentclass[table]{beamer}


\input{../preambles/standard_knitr_beamer_preamble}

%        The following variables are assumed by the standard preamble:
%	Global variable containing module name:
\title{Final concepts of SLR}
%	Global variable containing module shortname:
%		(Currently unused, may be used in future.)
\newcommand{\ModuleShortname}{introRegression}
%	Global variable containing author name:
\author{Nicholas G Reich, Jeff Goldsmith}
%	Global variable containing text of license terms:
\newcommand{\LicenseText}{Made available under the Creative Commons Attribution-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-sa/3.0/deed.en\textunderscore US }
%	Instructor: optional, can leave blank.
%		Recommended format: {Instructor: Jane Doe}
\newcommand{\Instructor}{}
%	Course: optional, can leave blank.
%		Recommended format: {Course: Biostatistics 101}
\newcommand{\Course}{}


\input{../preambles/shortcuts}

\hypersetup{colorlinks,linkcolor=,urlcolor=MainColor}


%	******	Document body begins here	**********************

\begin{document}

%	Title page
\begin{frame}[plain]
	\titlepage
\end{frame}

%	******	Everything through the above line must be placed at
%		the top of any TeX file using the statsTeachR standard
%		beamer preamble. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's lecture}

\bi
        \myitem Simple Linear Regression Continued
	\myitem Multiple Regression Intro
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Simple linear regression model}

\bi
	\myitem Observe data $(y_i, x_i)$ for subjects $1, \ldots, I$. Want to estimate $\beta_0, \beta_1$ in the model
	$$ y_i = \beta_0 + \beta_1x_i + \epsilon_i; \mbox{ } \epsilon_i \stackrel{iid}{\sim} (0,\sigma^2)$$
	\myitem Note the assumptions on the variance:
	\bi
		\myitem $E(\epsilon \mid x) = E(\epsilon) = 0$
		\myitem Constant variance
		\myitem Independence
		\myitem [Normally distributed is not needed for least squares, but is needed for inference]
	\ei
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Some definitions / SLR products}

\bi
	\myitem {\it Fitted values}: $\hat{y}_i := \hat{\beta}_0 + \hat{\beta}_1 x_i$
	\myitem {\it Residuals / estimated errors}: $\hat{\epsilon}_i := y_i - \hat{y}_i$
	\myitem {\it Residual sum of squares}: RSS := $\sum_{i = 1}^{n} \hat{\epsilon_i}^2$
	\myitem {\it Residual variance}: $\hat{\sigma^2} := \frac{RSS}{n-2}$
	\myitem {\it Degrees of freedom}: $n - 2$
\ei

Notes: residual sample mean is zero; residuals are uncorrelated with fitted values.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{$R^2$}

\begin{block}{Looking for a measure of goodness of fit.}

\bi
	\myitem RSS by itself doesn't work so well:
		$$ \sum_{i = 1}^{n} (y_i - \hat{y}_i )^2 $$
	\myitem Coefficient of determination ($R^2$) works better:
		$$ R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$$
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{$R^2$}

\begin{block}{Some notes about $R^2$}

\bi
	\myitem Interpreted as proportion of outcome variance explained by the model.
	\myitem Alternative form
		$$ R^2 = \frac{\sum (\hat{y}_i -\bar{y})^2}{\sum (y_i - \bar{y})^2}$$
	\myitem $R^2$ is bounded: $0 \leq R^2 \leq 1$
	\myitem For simple linear regression only, $R^2 = \rho^2$
\ei
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{ANOVA}

\begin{block}{Lots of sums of squares around.}

\bi
	\myitem Regression sum of squares $SS_{reg} = \sum (\hat{y}_i -\bar{y})^2$
	\myitem Residual sum of squares $SS_{res} = \sum (y_i - \hat{y}_i)^2$
	\myitem Total sum of squares $SS_{tot} = \sum (y_i - \bar{y})^2$
	\myitem All are related to sample variances
\ei

Analysis of variance (ANOVA) seeks to address goodness-of-fit by looking at these sample variances.
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{ANOVA}

ANOVA is based on the fact that $SS_{tot} = SS_{reg}+SS_{res}$

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{ANOVA}

ANOVA is based on the fact that $SS_{tot} = SS_{reg}+SS_{res}$

\begin{figure}[t]
    \includegraphics[width=.8\textwidth]{./Figs/Fig01.pdf}  
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{ANOVA and $R^2$}

\bi
	\myitem Both take advantage of sums of squares
	\myitem Both are defined for more complex models
	\myitem ANOVA can be used to derive a ``global hypothesis test" based on an F test
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}
\tiny
\begin{verbatim}
> linmod = lm(y~x)
> linmod

Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
     2.1123       0.6003  

\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}
\tiny
\begin{verbatim}
> linmod = lm(y~x)
> summary(linmod)

Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.3882 -0.6804 -0.3060  0.5997  1.6538 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.11232    0.25305   8.347 4.42e-09 ***
x            0.60031    0.05787  10.373 4.27e-11 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 0.8979 on 28 degrees of freedom
Multiple R-squared: 0.7935,	Adjusted R-squared: 0.7861 
F-statistic: 107.6 on 1 and 28 DF,  p-value: 4.266e-11 

\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}

\tiny
\begin{verbatim}
> names(linmod)
 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"   

\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}

\tiny
\begin{verbatim}
> linmod$residuals
          1           2           3           4           5           6 
-0.65795159  1.09467200  1.04653908  0.58695625  1.47428252  0.44508175 
          7           8           9          10          11          12 
-1.38817067 -0.68783957 -1.33798474 -0.58862243 -0.73403398 -0.07046559 
...
> linmod$fitted.values
        1         2         3         4         5         6         7         8 
8.2383435 3.8425638 5.1553944 3.9636613 2.5746592 4.2532308 0.6626529 6.5525729 
        9        10        11        12        13        14        15        16 
4.1892281 7.8259228 4.7695845 2.6346779 5.0130986 2.2310004 1.6555433 4.4381019 
...
\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}

\tiny
\begin{verbatim}

> names(summary(linmod))
 [1] "call"          "terms"         "residuals"     "coefficients" 
 [5] "aliased"       "sigma"         "df"            "r.squared"    
 [9] "adj.r.squared" "fstatistic"    "cov.unscaled" 
>
> summary(linmod)$coef
             Estimate Std. Error   t value     Pr(>|t|)
(Intercept) 2.1123158  0.2530506  8.347404 4.417462e-09
x           0.6003053  0.0578741 10.372607 4.265778e-11
>
> summary(linmod)$r.squared
[1] 0.7934966

\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}

\tiny
\begin{verbatim}

> anova(linmod)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(>F)    
x          1 86.744  86.744  107.59 4.266e-11 ***
Residuals 28 22.575   0.806                      
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
>
>
> 1- 22.575/86.744

\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Note on interpretation of $\beta_0$}

Recall $\beta_0 = E(y | x = 0)$
\bi
	\myitem This often makes no sense in context
	\myitem ``Centering" $x$ can be useful: $x^{*} = x - \bar{x}$
	\myitem Center by mean, median, minimum, etc
	\myitem Effect of centering on slope:
\ei



\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Note on interpretation of $\beta_0, \beta_1$}

\bi
	\myitem The interpretations are sensitive to the scale of the outcome and predictors (in reasonable ways)
	\myitem You can't get a better model fit by rescaling variables
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}

\tiny
\begin{verbatim}

> x.cen = x - mean(x)
> 
> linmod.cen = lm(y ~ x.cen)
> summary(linmod.cen)

Call:
lm(formula = y ~ x.cen)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.3882 -0.6804 -0.3060  0.5997  1.6538 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  4.11183    0.16394   25.08  < 2e-16 ***
x.cen        0.60031    0.05787   10.37 4.27e-11 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 0.8979 on 28 degrees of freedom
Multiple R-squared: 0.7935,	Adjusted R-squared: 0.7861 
F-statistic: 107.6 on 1 and 28 DF,  p-value: 4.266e-11 
\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{R example}

\tiny
\begin{verbatim}

> x2 = x*2
> 
> linmod.x2 = lm(y ~ x2)
> summary(linmod.x2)

Call:
lm(formula = y ~ x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.3882 -0.6804 -0.3060  0.5997  1.6538 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.11232    0.25305   8.347 4.42e-09 ***
x2           0.30015    0.02894  10.373 4.27e-11 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

Residual standard error: 0.8979 on 28 degrees of freedom
Multiple R-squared: 0.7935,	Adjusted R-squared: 0.7861 
F-statistic: 107.6 on 1 and 28 DF,  p-value: 4.266e-11 
\end{verbatim}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Properties of $\hat{\beta}_0, \hat{\beta}_1$}

\begin{figure}[t]
    \includegraphics[width=.8\textwidth]{./Figs/CircleOfLife.pdf}  
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Properties of $\hat{\beta}_0, \hat{\beta}_1$}

Estimates are unbiased:

$E(\hat{\beta_0}) = $

\vspace{2cm}

$E(\hat{\beta_1}) = $

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Properties of $\hat{\beta}_0, \hat{\beta}_1$}

Variances of estimates:

$Var(\hat{\beta_0}) = $

\vspace{2cm}

$Var(\hat{\beta_1}) = $

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Properties of $\hat{\beta}_0, \hat{\beta}_1$}

Note about the variance of $\beta_1$:
\bi
	\myitem Denominator contains $SS_{x} = \sum (x_i - \bar{x})^2$
	\myitem To decrease variance of $\beta_1$, increase variance of $x$
\ei

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{One slide on multiple linear regression}

\bi
	\myitem Observe data $(y_i, x_{i1}, \ldots, x_{ip})$ for subjects $1, \ldots, n$. Want to estimate $\beta_0, \beta_1, \ldots, \beta_p$ in the model
	$$ y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_1x_{ip} + \epsilon_i; \mbox{ } \epsilon_i \stackrel{iid}{\sim} (0,\sigma^2)$$
	\myitem Assumptions (residuals have mean zero, constant variance, are independent) are as in SLR
	\myitem Notation is cumbersome. To fix this, let
	\bi
		\myitem $\bx_i = [1, x_{i1}, \ldots, x_{ip}]$
		\myitem $\bbeta^{T} = [\beta_0, \beta_1, \ldots, \bbeta_{p}]$
		\myitem Then $y_i = \bx_i \bbeta + \epsilon_i$
	\ei
\ei

\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Today's big ideas}

\bi
	\myitem Simple linear regression definitions
	\myitem Properties of least squares estimates
\ei

\vspace{.75cm}
\hrule
\vspace{.75cm}

\bi
	\myitem Suggested reading: Faraway Ch 2.2 - 2.3
\ei

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}